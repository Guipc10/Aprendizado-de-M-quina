{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado de Máquina - Projeto 1\n",
    "### Guilherme Pereira Corrêa 198397\n",
    "### Bruno Moreira..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Métodos de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta e tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from time import perf_counter\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 2D padrão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primeiro visualizar os dados da tarefa 2D padrão e entender como os mesmos estão distribuídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scatter_data_2D(data):\n",
    "    plt.scatter(data[:,0],data[:,1])\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "def scatter_data_3D(data):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    ax.scatter3D(data[:,0],data[:,1],data[:,2])\n",
    "\n",
    "data = pd.read_csv('cluster.dat', delimiter=' ')\n",
    "np_data = data.to_numpy()\n",
    "print(np_data)\n",
    "scatter_data_2D(np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se, então, que a feature 1 deste conjunto de dados tem uma escala muito maior que a feature 2 e, portanto, pode afetar os resultados por ter um peso maior indesejado nos cálculos de distância que serão realizados. Escalar estes dados pode resolver isso. Além disso, os dados estão distruidos crescentemente e, portanto, devem ser embaralhados na separação entre train set e test set para evitar bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#normalizado com StandardScaler()\n",
    "data_scaled = StandardScaler().fit_transform(np_data)\n",
    "#normalizado com normalize\n",
    "data_normalized = normalize(np_data,axis=0)\n",
    "scatter_data_2D(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir uma função para separar o dataset em training set e test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data,train_proportion,test_proportion):\n",
    "    set_size = data.shape[0]\n",
    "    train_size = int(set_size*train_proportion)\n",
    "    train_set = data[:train_size]\n",
    "    test_set = data[train_size:]\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sem pré processamento\r\n",
    "np.random.shuffle(np_data)\r\n",
    "train_set,test_set = split_dataset(np_data,0.9,0.1)\r\n",
    "\r\n",
    "#Dados normalizados com StandardScaler\r\n",
    "np.random.shuffle(data_scaled)\r\n",
    "train_set_scaled,test_set_scaled = split_dataset(data_scaled,0.9,0.1)\r\n",
    "\r\n",
    "#Dados normalizados com normalize\r\n",
    "np.random.shuffle(data_normalized)\r\n",
    "train_set_normalized,test_set_normalized = split_dataset(data_normalized,0.9,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2º conjunto de dados: tweets.\n",
    "Foram escolhidos tweets para serem estudados como eles se comportam ao aplicar um algoritmo de clusterização. Porém, por se tratarem de textos, a forma como os mesmos são transformados em vetores de números (embedding) é crucial para o resultado. Neste projeto, esse processo foi dividido em três passos, onde existem diversas maneiras diferentes de se fazer cada um:\n",
    "\n",
    "1 - Coleta e pré processamento dos tweets.\n",
    "\n",
    "2 - Word embedding\n",
    "\n",
    "3 - Document embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Coleta e pré processamento dos tweets**\n",
    "\n",
    "O download dos tweets foi feito utilizando a biblioteca nltk, contendo 10000 tweets e uma anotação sobre o sentimento de cada um. Essa anotação é feita ao separar os mesmos em dois conjuntos, um para os tweets positivos e o outro para os tweets negativos, ou seja, como essa anotação não será utilizada neste trabalho, então esses dois conjuntos foram simplesmente concatenados um ao outro e então essa lista foi embaralhada a fim de evitar bias no momento de separar entre train set e test set.\n",
    "\n",
    "O pré processamento de cada tweet foi feito ao remover stopwords, pontuação, links e hashtags, além de tokenização. Não foi feito stemming pois o modelo de embeddings utilizado considera as palavras completas e não suas raizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):\n",
    "            # remove punctuation\n",
    "             tweets_clean.append(word)\n",
    "            #stem_word = stemmer.stem(word)  # stemming word\n",
    "            #tweets_clean.append(stem_word)\n",
    "    return tweets_clean\n",
    "\n",
    "def process_tweet_stem(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):\n",
    "            # remove punctuation\n",
    "             #tweets_clean.append(word)\n",
    "             stem_word = stemmer.stem(word)  # stemming word\n",
    "             tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "#Shuffle para não ter bias na hora de separar entre train_set e test_set\n",
    "np.random.shuffle(all_tweets)\n",
    "#Cria um dicionário que mapeia cada index à um tweet para consulta futura\n",
    "index2tweet_list = []\n",
    "for i,tweet in enumerate(all_tweets):\n",
    "    index2tweet_list.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Word embedding**\n",
    "\n",
    "Utilizando a biblioteca gensim, foi feito o download do modelo GloVe de 200 dimensões pré treinado no Twitter, essa escolha foi feita pois esse modelo é um dos que apresentam melhores resultados atualmente e, principalmente, porque ele foi treinado no Twitter, e o contexto de treino é muito importante neste tipo de tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina um modelo de embedding baseado no corpus existente, mas achamos mais conveniente usar um modelo pronto\n",
    "#embedding_model = Word2Vec(tweets_processed,vector_size=300,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding GloVe que será utilizado\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Document embedding**\n",
    "\n",
    "Há diversas maneiras de representar um tweet numericamente, mas muitas são custosas e, por conta disso, utilizamos uma ténica chamada Bag-of-words (BOW), onde a representação de um documento é dada pela soma das representações de cada palavra que o compõe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet, embedding_model,dimension):\n",
    "    tweet_embedding = np.zeros(dimension)\n",
    "    processed_tweet = process_tweet_stem(tweet)\n",
    "    print(processed_tweet)\n",
    "    for word in processed_tweet:\n",
    "        if embedding_model.get_index(word,False):\n",
    "            tweet_embedding += embedding_model.get_vector(word,norm=0)\n",
    "    return tweet_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tweets_embedded = [get_tweet_embedding(tweet,glove_vectors,200) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resta, então, separar entre conjunto de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma para numpy array\n",
    "tweets_embedded_np = np.array(tweets_embedded)\n",
    "#Dados sem nenhuma normalização ou pré processamento desse tipo\n",
    "tweets_train_set,tweets_test_set = split_dataset(tweets_embedded_np,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizados com StandardScaler()\n",
    "tweets_embedded_np_scaled = StandardScaler().fit_transform(tweets_embedded_np)\n",
    "tweets_train_set_scaled,tweets_test_set_scaled = split_dataset(tweets_embedded_np_scaled,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizados com normalize\n",
    "tweets_embedded_np_normalized = normalize(tweets_embedded_np, axis = 0)\n",
    "tweets_train_set_normalized,tweets_test_set_normalized = split_dataset(tweets_embedded_np_normalized,0.9,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "source": [
    "Classes e funções implementadas para aplicar o k-means:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, dimensions, point_index, cluster_index):\n",
    "        self.dimensions = dimensions\n",
    "        self.point_index = point_index\n",
    "        self.cluster_index = cluster_index\n",
    "        self.distance_to_center = -1\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, dimensions):\n",
    "        self.dimensions = dimensions\n",
    "        self.points = []\n",
    "\n",
    "def k_means(data, k):\n",
    "    clusters = forgy_cluster_initialization(k, data)\n",
    "    redefined = 1\n",
    "    interac = 0\n",
    "    while redefined == 1:\n",
    "        if (interac%500 == 0):\n",
    "            print(\"interaction number \", interac)\n",
    "        for point in data:\n",
    "            for i in range(len(clusters)):\n",
    "                if (point.cluster_index == -1):\n",
    "                    point.cluster_index = i\n",
    "                cluster_from_current_point = clusters[point.cluster_index]\n",
    "                if (distance.euclidean(point.dimensions, clusters[i].dimensions) < distance.euclidean(point.dimensions, cluster_from_current_point.dimensions)):\n",
    "                    point.cluster_index = i\n",
    "                    point.distance_to_center = distance.euclidean(point.dimensions, clusters[i].dimensions)\n",
    "            clusters[point.cluster_index].points.append(point)\n",
    "        redefined = redefine_clusters_center(clusters)\n",
    "        interac += 1\n",
    "    return clusters\n",
    "    \n",
    "def random_initialization(k, data):\n",
    "    data_points = []\n",
    "    for point in data:\n",
    "        data_points.append(point.dimensions)\n",
    "\n",
    "    max_value = np.max(np.array(data_points)[:,0])\n",
    "    d = len(point.dimensions)\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        clusters.append(Cluster([random.uniform(0, max_value)] * d))\n",
    "    return clusters\n",
    "\n",
    "def forgy_cluster_initialization(k, data):\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        random_index = random.randint(0, len(data))\n",
    "        clusters.append(Cluster(data[random_index].dimensions))\n",
    "    return clusters\n",
    "\n",
    "def redefine_clusters_center(clusters):\n",
    "    redefined = 0\n",
    "    for cluster in clusters:\n",
    "        if (len(cluster.points) == 0):\n",
    "            continue\n",
    "\n",
    "        d = len(cluster.dimensions)\n",
    "        sums = [0] * d\n",
    "\n",
    "        for point in cluster.points:\n",
    "            for i in range(d):\n",
    "                sums[i] += point.dimensions[i]\n",
    "\n",
    "        means = [0] * d\n",
    "        n = len(cluster.points)\n",
    "        for i in range(d):\n",
    "            means[i] = round(sums[i]/n, 300)\n",
    "            if (means[i] - cluster.dimensions[i] > 0.000001):\n",
    "                cluster.dimensions[i] = means[i]\n",
    "                redefined = 1\n",
    "        if (redefined == 1):\n",
    "            cluster.points.clear()\n",
    "    return redefined\n",
    "\n",
    "def plot_2d_data(clusters):\n",
    "    colors = [\"green\",\"blue\",\"yellow\",\"pink\",\"orange\",\"purple\",\"beige\",\"brown\",\"gray\",\"cyan\",\"magenta\", \"black\"]\n",
    "    for cluster in clusters:\n",
    "        plt.plot(cluster.dimensions[0], cluster.dimensions[1], color = 'red', marker = 'o')\n",
    "        for point in cluster.points:\n",
    "            plt.scatter(point.dimensions[0], point.dimensions[1], color = colors[point.cluster_index], alpha = 0.5)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "def elbow_method(min_k, max_k, data):\n",
    "    costs = []\n",
    "    for i in range(min_k, (max_k + 1)):\n",
    "        clusters = k_means(data, i)\n",
    "        costs.append(cost_evaluation(clusters))\n",
    "    plt.plot(range(1, max_k + 1), costs, color = 'blue', marker = 'o', alpha = 0.5)\n",
    "    plt.xlabel('No. clusters')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def silhouette_analysis():\n",
    "    return\n",
    "\n",
    "def cost_evaluation(clusters):\n",
    "    j = 0\n",
    "    for cluster in clusters:\n",
    "        for point in cluster.points:\n",
    "            j += pow(point.distance_to_center, 2)\n",
    "    return j\n",
    "def print_tweets_clusters(clusters, all_tweets_table):\n",
    "    for i in range(len(clusters)):\n",
    "        print('Cluster número ', i)\n",
    "        for j in range(10):\n",
    "            #Imprime 10 tweets aleatorios que pertencem ao cluster i\n",
    "            print(all_tweets_table[clusters[i].points[random.randrange(0,len(clusters[i].points))].point_index])\n",
    "        print('--------próximo cluster----------')\n",
    "    \n",
    "def print_word_cloud(clusters,all_tweets_table):\n",
    "    tweets_processed = []\n",
    "    for i in range(len(all_tweets_table)):\n",
    "        tweets_processed.append(process_tweet_stem(all_tweets_table[i]))\n",
    "    for i in range(len(clusters)):\n",
    "        unique_strings = ''\n",
    "        print('Cluster número ', i, ' tamanho:', len(clusters[i].points))\n",
    "        for j in range(len(clusters[i].points)):\n",
    "            unique_strings += ' '+(' ').join(tweets_processed[clusters[i].points[j].point_index])\n",
    "        wordcloud = WordCloud(width = 1000, height = 500).generate(unique_strings)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print('--------próximo cluster----------')"
   ]
  },
  {
   "source": [
    "**Tarefa 2D padrão:**\n",
    "\n",
    "Será feito o teste com os dados normalizados com a função normalize do sklearn, StandardScaler e sem realizar nenhum ajuste."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar nos sets de treino\n",
    "#Dados sem pré processamento:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set)):\n",
    "    points.append(Point(train_set[i], i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dados normalizados com StandardScaler\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set_scaled)):\n",
    "    points.append(Point(train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizador com normalize\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set_normalized)):\n",
    "    points.append(Point(train_set_normalized[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados"
   ]
  },
  {
   "source": [
    "Percebe-se então que os dois métodos de normalização fizeram com que o resultado fosse o esperado, enquanto que a técnica aplicada nos dados sem processamento gerou clusters longe do esperado e um custo muito alto. Além disso, percebe-se também que o custo utilizando a função normalize foi menor. Esse resultado faz sentido pois a normalização apresenta melhores resultados quando comparada ao StandardScaler quando os dados do problema não seguem uma distruibuição Gaussiana."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**2º conjunto de dados: tweets.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Será feito o teste com os dados normalizados com a função normalize do sklearn, StandardScaler e sem realizar nenhum ajuste."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sem pré processamento:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set)):\n",
    "    points.append(Point(tweets_train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_tweets_clusters(clusters, all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando StandardScaler:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set_scaled)):\n",
    "    points.append(Point(tweets_train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_tweets_clusters(clusters, all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando normalize:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set_normalized)):\n",
    "    points.append(Point(tweets_train_set_normalized[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_tweets_clusters(clusters, all_tweets)"
   ]
  },
  {
   "source": [
    "Por conta da representação dos tweets ser um dos possíveis motivos da clusterização não ter tido um resultado muito bom, será testada a representação Doc2Vec, que considera o tweet como um todo na representação e não faz somente uma média da representação de cada palavra do mesmo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(process_tweet_stem(doc), [i]) for i, doc in enumerate(all_tweets)]\n",
    "model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_model_data = [model.dv[i] for i in range(len(all_tweets))]\n",
    "document_model_data_normalized = normalize(np.array(document_model_data),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Usando normalize:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(document_model_data_normalized)):\n",
    "    points.append(Point(document_model_data_normalized[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "for i,cluster in enumerate(clusters):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "print_word_cloud(clusters,all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print_word_cloud(clusters,all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(document_model_data_normalized)):\n",
    "    points.append(Point(document_model_data_normalized[i],i, -1))\n",
    "elbow_method(1,10,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(process_tweet_stem(doc), [i]) for i, doc in enumerate(all_tweets)]\n",
    "model_300 = Doc2Vec(documents, vector_size=300, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_model_data_300 = [model_300.dv[i] for i in range(len(all_tweets))]\n",
    "document_model_data_normalized_300 = normalize(np.array(document_model_data_300),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_model_data_normalized_300)"
   ]
  },
  {
   "source": [
    "### Outro método: DBSCAN\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "O método DBSCAN foi escolhido para ser implementado, já que o mesmo lida bem com outliers e o dataset dos tweets possui alguns, o que pode ter influenciado nos resultados. As classes e funções implementadas foram as seguintes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANpoint:\n",
    "    def __init__(self,dimensions,index):\n",
    "        self.dimensions = dimensions\n",
    "        self.point_index = index\n",
    "        self.cluster = None\n",
    "        self.category = 'NONE'\n",
    "        self.neighbours = []\n",
    "        self.n_neighbours = 0\n",
    "        self.there_is_core_in_neighbourhood = False\n",
    "\n",
    "    def add_neighbour(self,DBSCANpoint):\n",
    "        self.neighbours.append(DBSCANpoint)\n",
    "        self.n_neighbours += 1\n",
    "\n",
    "class DBSCANCluster:\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "\n",
    "def dbscan(data,epsilon,M):\n",
    "    #data is a np.array\n",
    "    all_points,core_points,outliers = define_type(data,epsilon,M)\n",
    "    clusters = []\n",
    "    #form clusters\n",
    "    for core_point in core_points:\n",
    "        #Connect all core_points\n",
    "        for neighbour in core_point.neighbours:\n",
    "            if neighbour.category == 'CORE':\n",
    "                if neighbour.cluster == None:\n",
    "                    #neighbour core point still dont have a cluster_index\n",
    "                    if (core_point.cluster == None):\n",
    "                        #both of them dont have a cluster index, so assign them a new one\n",
    "                        core_point.cluster = DBSCANCluster()\n",
    "                        neighbour.cluster = core_point.cluster\n",
    "                        clusters.append(core_point.cluster)\n",
    "                        core_point.cluster.points.append(core_point)\n",
    "                        core_point.cluster.points.append(neighbour)\n",
    "                    else:\n",
    "                        #core_point already has a cluster    \n",
    "                        neighbour.cluster = core_point.cluster\n",
    "                        core_point.cluster.points.append(neighbour)\n",
    "                else:\n",
    "                    #neighbour already has a cluster\n",
    "                    if (neighbour.cluster != core_point.cluster):\n",
    "                        #print('vizinho já tem cluster e é diferente do outro')\n",
    "                        #if (neighbour.cluster in clusters):\n",
    "                        #    print('E ta na lista!')\n",
    "                        merge_clusters(clusters,core_point.cluster,neighbour.cluster)\n",
    "                        \n",
    "\n",
    "    for core_point in core_points:\n",
    "        #Assign clusters to the border points\n",
    "        for neighbour in core_point.neighbours:\n",
    "            if neighbour.category == 'CORE':\n",
    "                continue\n",
    "            neighbour.cluster = core_point.cluster\n",
    "            core_point.cluster.points.append(neighbour)\n",
    "    return clusters,outliers\n",
    "\n",
    "def define_type(data,epsilon,M):\n",
    "    points = []\n",
    "    core_points = []\n",
    "    outliers = []\n",
    "    for i in range(0,len(data)):\n",
    "        points.append(DBSCANpoint(data[i],i))\n",
    "    \n",
    "    for point in points:\n",
    "        search_neighbours(point,points,epsilon)\n",
    "        if point.n_neighbours >= M:\n",
    "            point.category = 'CORE'\n",
    "            core_points.append(point)\n",
    "        elif point.there_is_core_in_neighbourhood:\n",
    "            point.category = 'BORDER'\n",
    "        else:\n",
    "            point.category = 'OUTLIER'\n",
    "            outliers.append(point)\n",
    "    \n",
    "    return points,core_points,outliers\n",
    "\n",
    "def search_neighbours(DBSCANpoint, all_points,epsilon):\n",
    "    for point in all_points:\n",
    "        if (distance.euclidean(DBSCANpoint.dimensions,point.dimensions) <= epsilon):\n",
    "            DBSCANpoint.add_neighbour(point)\n",
    "            if point.category == 'CORE':\n",
    "                DBSCANpoint.there_is_core_in_neighbourhood = True\n",
    "\n",
    "def merge_clusters(clusters_list,cluster_1,cluster_2):\n",
    "    #all points of cluster 2 are gonna become part of cluster 1\n",
    "    for point in cluster_2.points:\n",
    "        point.cluster = cluster_1\n",
    "        cluster_1.points.append(point)\n",
    "        cluster_2.points.remove(point)\n",
    "        #print(\"ponto removido, agora o tamanho dos pontos do cluster 2 é \", len(cluster_2.points))\n",
    "    if (cluster_2 in clusters_list):\n",
    "         clusters_list.remove(cluster_2)\n",
    "   \n",
    "\n",
    "def dbscan_plot_2d_data(clusters):\n",
    "    colors = [\"green\",\"blue\",\"yellow\",\"pink\",\"orange\",\"purple\",\"beige\",\"brown\",\"gray\",\"cyan\",\"magenta\", \"black\"]\n",
    "    for cluster in clusters:\n",
    "        for point in cluster.points:\n",
    "            plt.scatter(point.dimensions[0], point.dimensions[1], color = colors[clusters.index(point.cluster)], alpha = 0.5)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "M = 5\n",
    "epsilon = 0.005\n",
    "clusters_dbscan_2d,_ = dbscan(train_set_normalized, epsilon, M)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "for i,cluster in enumerate(clusters):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "dbscan_plot_2d_data(clusters_dbscan_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-a8b39a364b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclusters_dbscan_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutliers_normalized\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_model_data_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-a3f65d171574>\u001b[0m in \u001b[0;36mdbscan\u001b[1;34m(data, epsilon, M)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#data is a np.array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mall_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcore_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutliers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#form clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-a3f65d171574>\u001b[0m in \u001b[0;36mdefine_type\u001b[1;34m(data, epsilon, M)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0msearch_neighbours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_neighbours\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CORE'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-a3f65d171574>\u001b[0m in \u001b[0;36msearch_neighbours\u001b[1;34m(DBSCANpoint, all_points, epsilon)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msearch_neighbours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDBSCANpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_points\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDBSCANpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mDBSCANpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_neighbour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'CORE'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \"\"\"\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \"\"\"\n\u001b[1;32m--> 513\u001b[1;33m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36m_validate_vector\u001b[1;34m(u, dtype)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;31m# Ensure values such as u=1 and u=[1] still return 1-D arrays.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input vector should be 1-D.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = perf_counter()\n",
    "M = 20\n",
    "epsilon = 0.2\n",
    "clusters_dbscan_normalized, outliers_normalized= dbscan(document_model_data_normalized, epsilon, M)\n",
    "toc = perf_counter()\n",
    "total_time = (toc-tic)*1000\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cluster in enumerate(clusters_dbscan_normalized):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "print_word_cloud(clusters_dbscan_normalized,all_tweets)\n",
    "print('Outliers len: ',len(outliers_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_model_data_standard = StandardScaler().fit_transform(np.array(document_model_data))\r\n",
    "tic = perf_counter() \r\n",
    "M = 20\r\n",
    "epsilon = 0.05\r\n",
    "clusters_dbscan_scaled,outliers_scaled = dbscan(document_model_data_standard, epsilon, M)\r\n",
    "toc = perf_counter()\r\n",
    "total_time = (toc-tic)*1000\r\n",
    "print (f\"It took {total_time:.2f}ms to run the algorithm.\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cluster in enumerate(clusters_dbscan_scaled):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "print_word_cloud(clusters_dbscan_scaled,all_tweets)\n",
    "print('Outliers len: ',len(outliers_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - Redução de dimensionalidade"
   ]
  },
  {
   "source": [
    "A biblioteca scikit lean foi utilizada para aplicar PCA aos dados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def red_dim(data,n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(data)\n",
    "    new_data = pca.transform(data)\n",
    "    print(\"Dimensionalidade reduzida, proporção de variância em cada componente é:\")\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    return new_data\n",
    "\n",
    "#\n",
    "tweets_new_2D = red_dim(tweets_embedded_np_normalized_2,2)\n",
    "scatter_data_2D(tweets_new_2D)\n",
    "tweets_new_3D = red_dim(tweets_embedded_np_normalized_2,3)\n",
    "scatter_data_3D(tweets_new_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means nos tweets em 2D\n"
   ]
  },
  {
   "source": [
    "Percebe-se então"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd030e434770c8343143e074606db59223f9c01b362ce1f6346061549c91b66ccf6",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}