{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado de Máquina - Projeto 1\n",
    "### Guilherme Pereira Corrêa 198397\n",
    "### Bruno Moreira..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Métodos de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta e tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from time import perf_counter\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 2D padrão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primeiro visualizar os dados da tarefa 2D padrão e entender como os mesmos estão distribuídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scatter_data_2D(data):\n",
    "    plt.scatter(data[:,0],data[:,1])\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "def scatter_data_3D(data):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    ax.scatter3D(data[:,0],data[:,1],data[:,2])\n",
    "\n",
    "data = pd.read_csv('cluster.dat', delimiter=' ')\n",
    "np_data = data.to_numpy()\n",
    "print(np_data)\n",
    "scatter_data_2D(np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se, então, que a feature 1 deste conjunto de dados tem uma escala muito maior que a feature 2 e, portanto, pode afetar os resultados por ter um peso maior indesejado nos cálculos de distância que serão realizados. Escalar estes dados pode resolver isso. Além disso, os dados estão distruidos crescentemente e, portanto, devem ser embaralhados na separação entre train set e test set para evitar bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#normalizado com StandardScaler()\n",
    "data_scaled = StandardScaler().fit_transform(np_data)\n",
    "#normalizado com normalize\n",
    "data_normalized = normalize(np_data,axis=0)\n",
    "scatter_data_2D(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir uma função para separar o dataset em training set e test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data,train_proportion,test_proportion):\n",
    "    set_size = data.shape[0]\n",
    "    train_size = int(set_size*train_proportion)\n",
    "    train_set = data[:train_size]\n",
    "    test_set = data[train_size:]\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sem pré processamento\r\n",
    "np.random.shuffle(np_data)\r\n",
    "train_set,test_set = split_dataset(np_data,0.9,0.1)\r\n",
    "\r\n",
    "#Dados normalizados com StandardScaler\r\n",
    "np.random.shuffle(data_scaled)\r\n",
    "train_set_scaled,test_set_scaled = split_dataset(data_scaled,0.9,0.1)\r\n",
    "\r\n",
    "#Dados normalizados com normalize\r\n",
    "np.random.shuffle(data_normalized)\r\n",
    "train_set_normalized,test_set_normalized = split_dataset(data_normalized,0.9,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2º conjunto de dados: tweets.\n",
    "Foram escolhidos tweets para serem estudados como eles se comportam ao aplicar um algoritmo de clusterização. Porém, por se tratarem de textos, a forma como os mesmos são transformados em vetores de números (embedding) é crucial para o resultado. Neste projeto, esse processo foi dividido em três passos, onde existem diversas maneiras diferentes de se fazer cada um:\n",
    "\n",
    "1 - Coleta e pré processamento dos tweets.\n",
    "\n",
    "2 - Word embedding\n",
    "\n",
    "3 - Document embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Coleta e pré processamento dos tweets**\n",
    "\n",
    "O download dos tweets foi feito utilizando a biblioteca nltk, contendo 10000 tweets e uma anotação sobre o sentimento de cada um. Essa anotação é feita ao separar os mesmos em dois conjuntos, um para os tweets positivos e o outro para os tweets negativos, ou seja, como essa anotação não será utilizada neste trabalho, então esses dois conjuntos foram simplesmente concatenados um ao outro e então essa lista foi embaralhada a fim de evitar bias no momento de separar entre train set e test set.\n",
    "\n",
    "O pré processamento de cada tweet foi feito ao remover stopwords, pontuação, links e hashtags, além de tokenização. A princípio não foi feito stemming pois planejava-se usar embedder pronto e o mesmo considera as palavras completas e não suas raizes. Porém, por conta da possibilidade de utilizar outro embedder estar em aberta, foram implementados duas funções de pré processamento, uma com stemming, e uma sem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):\n",
    "            # remove punctuation\n",
    "             tweets_clean.append(word)\n",
    "            #stem_word = stemmer.stem(word)  # stemming word\n",
    "            #tweets_clean.append(stem_word)\n",
    "    return tweets_clean\n",
    "\n",
    "def process_tweet_stem(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):\n",
    "            # remove punctuation\n",
    "             #tweets_clean.append(word)\n",
    "             stem_word = stemmer.stem(word)  # stemming word\n",
    "             tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "source": [
    "Download do tweets e embaralhamento dos mesmos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "#Shuffle para não ter bias na hora de separar entre train_set e test_set\n",
    "np.random.shuffle(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Word embedding**\n",
    "\n",
    "Utilizando a biblioteca gensim, foi feito o download do modelo GloVe de 200 dimensões pré treinado no Twitter, essa escolha foi feita pois esse modelo é um dos que apresentam melhores resultados atualmente e, principalmente, porque ele foi treinado no Twitter, e o contexto de treino é muito importante neste tipo de tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding GloVe que será utilizado\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Document embedding**\n",
    "\n",
    "Há diversas maneiras de representar um tweet numericamente, mas muitas são custosas e, por conta disso, utilizamos uma ténica chamada Bag-of-words (BOW), onde a representação de um documento é dada pela soma das representações de cada palavra que o compõe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet, embedding_model,dimension):\n",
    "    tweet_embedding = np.zeros(dimension)\n",
    "    processed_tweet = process_tweet_stem(tweet)\n",
    "    print(processed_tweet)\n",
    "    for word in processed_tweet:\n",
    "        if embedding_model.get_index(word,False):\n",
    "            tweet_embedding += embedding_model.get_vector(word,norm=0)\n",
    "    return tweet_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tweets_embedded = [get_tweet_embedding(tweet,glove_vectors,200) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resta, então, separar entre conjunto de treino e teste, será feito teste com estes dados sem pré processamento, escalados com StandardScaler() e normalizados com normalize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma para numpy array\n",
    "tweets_embedded_np = np.array(tweets_embedded)\n",
    "#Dados sem nenhuma normalização ou pré processamento desse tipo\n",
    "tweets_train_set,tweets_test_set = split_dataset(tweets_embedded_np,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizados com StandardScaler()\n",
    "tweets_embedded_np_scaled = StandardScaler().fit_transform(tweets_embedded_np)\n",
    "tweets_train_set_scaled,tweets_test_set_scaled = split_dataset(tweets_embedded_np_scaled,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizados com normalize\n",
    "tweets_embedded_np_normalized = normalize(tweets_embedded_np, axis = 0)\n",
    "tweets_train_set_normalized,tweets_test_set_normalized = split_dataset(tweets_embedded_np_normalized,0.9,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "source": [
    "Classes e funções implementadas para aplicar o k-means:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, dimensions, point_index, cluster_index):\n",
    "        self.dimensions = dimensions\n",
    "        self.point_index = point_index\n",
    "        self.cluster_index = cluster_index\n",
    "        self.distance_to_center = -1\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, dimensions):\n",
    "        self.dimensions = dimensions\n",
    "        self.points = []\n",
    "\n",
    "def k_means(data, k):\n",
    "    clusters = forgy_cluster_initialization(k, data)\n",
    "    redefined = 1\n",
    "    interac = 0\n",
    "    while redefined == 1:\n",
    "        if (interac%500 == 0):\n",
    "            print(\"interaction number \", interac)\n",
    "        for point in data:\n",
    "            for i in range(len(clusters)):\n",
    "                if (point.cluster_index == -1):\n",
    "                    point.cluster_index = i\n",
    "                cluster_from_current_point = clusters[point.cluster_index]\n",
    "                if (distance.euclidean(point.dimensions, clusters[i].dimensions) < distance.euclidean(point.dimensions, cluster_from_current_point.dimensions)):\n",
    "                    point.cluster_index = i\n",
    "                    point.distance_to_center = distance.euclidean(point.dimensions, clusters[i].dimensions)\n",
    "            clusters[point.cluster_index].points.append(point)\n",
    "        redefined = redefine_clusters_center(clusters)\n",
    "        interac += 1\n",
    "    return clusters\n",
    "    \n",
    "def random_initialization(k, data):\n",
    "    data_points = []\n",
    "    for point in data:\n",
    "        data_points.append(point.dimensions)\n",
    "\n",
    "    max_value = np.max(np.array(data_points)[:,0])\n",
    "    d = len(point.dimensions)\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        clusters.append(Cluster([random.uniform(0, max_value)] * d))\n",
    "    return clusters\n",
    "\n",
    "def forgy_cluster_initialization(k, data):\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        random_index = random.randint(0, len(data))\n",
    "        clusters.append(Cluster(data[random_index].dimensions))\n",
    "    return clusters\n",
    "\n",
    "def redefine_clusters_center(clusters):\n",
    "    redefined = 0\n",
    "    for cluster in clusters:\n",
    "        if (len(cluster.points) == 0):\n",
    "            continue\n",
    "\n",
    "        d = len(cluster.dimensions)\n",
    "        sums = [0] * d\n",
    "\n",
    "        for point in cluster.points:\n",
    "            for i in range(d):\n",
    "                sums[i] += point.dimensions[i]\n",
    "\n",
    "        means = [0] * d\n",
    "        n = len(cluster.points)\n",
    "        for i in range(d):\n",
    "            means[i] = round(sums[i]/n, 300)\n",
    "            if (means[i] - cluster.dimensions[i] > 0.000001):\n",
    "                cluster.dimensions[i] = means[i]\n",
    "                redefined = 1\n",
    "        if (redefined == 1):\n",
    "            cluster.points.clear()\n",
    "    return redefined\n",
    "\n",
    "def plot_2d_data(clusters):\n",
    "    colors = [\"green\",\"blue\",\"yellow\",\"pink\",\"orange\",\"purple\",\"beige\",\"brown\",\"gray\",\"cyan\",\"magenta\", \"black\"]\n",
    "    for cluster in clusters:\n",
    "        plt.plot(cluster.dimensions[0], cluster.dimensions[1], color = 'red', marker = 'o')\n",
    "        for point in cluster.points:\n",
    "            plt.scatter(point.dimensions[0], point.dimensions[1], color = colors[point.cluster_index], alpha = 0.5)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "def elbow_method(min_k, max_k, data):\n",
    "    costs = []\n",
    "    for i in range(min_k, (max_k + 1)):\n",
    "        clusters = k_means(data, i)\n",
    "        costs.append(cost_evaluation(clusters))\n",
    "    plt.plot(range(1, max_k + 1), costs, color = 'blue', marker = 'o', alpha = 0.5)\n",
    "    plt.xlabel('No. clusters')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def silhouette_analysis():\n",
    "    return\n",
    "\n",
    "def cost_evaluation(clusters):\n",
    "    j = 0\n",
    "    for cluster in clusters:\n",
    "        for point in cluster.points:\n",
    "            j += pow(point.distance_to_center, 2)\n",
    "    return j\n",
    "def print_tweets_clusters(clusters, all_tweets_table):\n",
    "    for i in range(len(clusters)):\n",
    "        print('Cluster número ', i)\n",
    "        for j in range(10):\n",
    "            #Imprime 10 tweets aleatorios que pertencem ao cluster i\n",
    "            print(all_tweets_table[clusters[i].points[random.randrange(0,len(clusters[i].points))].point_index])\n",
    "        print('--------próximo cluster----------')\n",
    "    \n"
   ]
  },
  {
   "source": [
    "**Tarefa 2D padrão:**\n",
    "\n",
    "Será feito o teste com os dados normalizados com a função normalize do sklearn, StandardScaler e sem realizar nenhum ajuste. Além disso, como métrica de avaliação será plotado uma análise dos coeficientes de Silhouette, o valor médio do mesmo e também a função custo, que é dada pela soma da distância ao quadrado de cada ponto ao centro do seu respectivo cluster. Desta forma é possível avaliar os clusters internamente (densidade) e externamente (separação), pois ambos são englobados pelo coeficiente de silhouette, além da função custo, que é uma outra forma de avaliar a densidade dos clusters. Os outros testes feitos ao longo do trabalho também seguirão esse padrão. Apesar da função custo não representar com a mesma precisão a densidade dos clusters quando o método utilizado faz o agrupamento em outros formatos além do esférico, essa métrica ainda foi mantida para fins de comparação."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar nos sets de treino\n",
    "#Dados sem pré processamento:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set)):\n",
    "    points.append(Point(train_set[i], i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_2D_sp = (toc-tic)*1000\n",
    "print (f\"It took {time_2D_sp:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)\n",
    "avg_sa_2D_sp = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dados normalizados com StandardScaler\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set_scaled)):\n",
    "    points.append(Point(train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_2D_standard = (toc-tic)*1000\n",
    "print (f\"It took {time_2D_standard:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)\n",
    "avg_sa_2D_standard = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizador com normalize\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(train_set_normalized)):\n",
    "    points.append(Point(train_set_normalized[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_2D_normalized = (toc-tic)*1000\n",
    "print (f\"It took {time_2D_normalized:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "plot_2d_data(clusters)\n",
    "avg_sa_2D_normalized = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados"
   ]
  },
  {
   "source": [
    "Percebe-se então que os dois métodos de normalização fizeram com que o resultado fosse o esperado, enquanto que a técnica aplicada nos dados sem processamento gerou clusters longe do esperado e um custo muito alto. Além disso, percebe-se também que o custo utilizando a função normalize foi menor. Esse resultado faz sentido pois a normalização apresenta melhores resultados quando comparada ao StandardScaler quando os dados do problema não seguem uma distruibuição Gaussiana."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Nos testes feitos anteriormente assumiu-se k = 3 por saber-se previamente o formato dos dados, mas esse não será o caso no próximo conjunto de dados, portanto foi implementado o método do elbow para estimativa do k. É necessário, então, testá-lo no conjunto de dados em 2D antes de aplicá-lo nos dados mais numerosos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(train_set_normalized)):\n",
    "    points.append(Point(train_set_normalized[i],i, -1))\n",
    "elbow_method(1,10,points)"
   ]
  },
  {
   "source": [
    "Percebe-se, então, que k = 3 é o melhor valor para k de acordo com o método do elbow e, portanto, o método aparenta ser eficaz e pode ser utilizado no segundo conjunto de dados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**2º conjunto de dados: tweets.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Será feito o teste com os dados normalizados com a função normalize do sklearn, StandardScaler e sem realizar nenhum ajuste. Primeiramente será rodado o algoritmo com k = 2 a fim de encontrar a melhor forma de pré processar os dados, já que sabe-se a princípio que há tweets positivos e negativos no conjunto e esse pode ser um bom número de clusters. Em seguida, será rodado o método do elbow para verificar se essa hipótese é verdadeira. Sabe-se, também, que a clusterização nestes dados é extremamente dependente da forma como embedding foi feito e, por conta disso, poderá ser testado um outro embedding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Os clusters resultantes serão analisados ao gerar word clouds, onde as palavras mais frequentes de cada clusters serão impressas em um tamanho maior, possibilitando, então, identificar possíveis tópicos predominantes em cada cluster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_cloud(clusters,all_tweets_table):\n",
    "    tweets_processed = []\n",
    "    for i in range(len(all_tweets_table)):\n",
    "        tweets_processed.append(process_tweet_stem(all_tweets_table[i]))\n",
    "    for i in range(len(clusters)):\n",
    "        unique_strings = ''\n",
    "        print('Cluster número ', i, ' tamanho:', len(clusters[i].points))\n",
    "        for j in range(len(clusters[i].points)):\n",
    "            unique_strings += ' '+(' ').join(tweets_processed[clusters[i].points[j].point_index])\n",
    "        wordcloud = WordCloud(width = 1000, height = 500).generate(unique_strings)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print('--------próximo cluster----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sem pré processamento:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set)):\n",
    "    points.append(Point(tweets_train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_sp = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_sp:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_word_cloud(clusters,all_tweets)\n",
    "avg_sa_tweets_sp = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando StandardScaler:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set_scaled)):\n",
    "    points.append(Point(tweets_train_set_scaled[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_standard = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_standard:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_word_cloud(clusters,all_tweets)\n",
    "avg_sa_tweets_standard = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando normalize:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(tweets_train_set_normalized)):\n",
    "    points.append(Point(tweets_train_set_normalized[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_normalized = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_normalized:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_word_cloud(clusters,all_tweets)\n",
    "avg_sa_tweets_normalized = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "source": [
    "Por conta da representação dos tweets ser um dos possíveis motivos da clusterização não ter tido um resultado muito bom, será testada a representação Doc2Vec, que considera o tweet como um todo na representação e não faz somente uma média da representação de cada palavra do mesmo. Além disso, esta representação será treinada dentro do nosso próprio conjunto de dados, onde espera-se um resultado mais satisfatório, porém um número maior de tweets seria necessário para uma representação mais precisa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(process_tweet_stem(doc), [i]) for i, doc in enumerate(all_tweets)]\n",
    "model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_model_data = [model.dv[i] for i in range(len(all_tweets))]\n",
    "document_model_data_normalized = normalize(np.array(document_model_data),axis=0)\n",
    "#Separando entre conjunto de teste e treino\n",
    "document_model_train_data_normalized, document_model_test_data_normalized = split_dataset(document_model_data_normalized,0.9,0.1)"
   ]
  },
  {
   "source": [
    "Primeiro será rodado o método do elbow para encontrar o melhor valor para k."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(document_model_train_data_normalized)):\n",
    "    points.append(Point(document_model_train_data_normalized[i],i, -1))\n",
    "elbow_method(1,10,points)"
   ]
  },
  {
   "source": [
    "Percebe-se então que k=3 é o valor mais indicado."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Usando normalize:\n",
    "tic = perf_counter()\n",
    "points = []\n",
    "for i in range(len(document_model_train_data_normalized)):\n",
    "    points.append(Point(document_model_train_data_normalized[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_document = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_document:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "for i,cluster in enumerate(clusters):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "print_word_cloud(clusters,all_tweets)\n",
    "avg_sa_tweets_document = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "source": [
    "### Outro método: DBSCAN\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "O método DBSCAN foi escolhido para ser implementado, já que o mesmo lida bem com outliers e o dataset dos tweets possui alguns, o que pode ter influenciado nos resultados.\n",
    "\n",
    "Assim como no k-means, foram definidas as classes DBSCANpoint e DBSCANCluster para facilitarem na implementação do algoritmo. A ckasse DBSCANpoint, utilizada para representar um dado, contém as dimensões do ponto (x,y,z,...), o identificador de cada ponto (utilizado para recuperar os tweets), o cluster a qual ele pertence, sua categoria no DBSCAN (core, outlier ou border), seus vizinhos, o número total de vizinhos, informação sobre a presença de um core point na vizinhança e, por fim, o coeficiente de silhouette associado.\n",
    "\n",
    "A classe DBSCANCluster, por sua vez, possui somente uma lista "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANpoint:\n",
    "    def __init__(self,dimensions,index):\n",
    "        self.dimensions = dimensions\n",
    "        self.point_index = index\n",
    "        self.cluster = None\n",
    "        self.category = 'NONE'\n",
    "        self.neighbours = []\n",
    "        self.n_neighbours = 0\n",
    "        self.there_is_core_in_neighbourhood = False\n",
    "        self.silhouette_coefficient = 0\n",
    "\n",
    "    def add_neighbour(self,DBSCANpoint):\n",
    "        self.neighbours.append(DBSCANpoint)\n",
    "        self.n_neighbours += 1\n",
    "\n",
    "class DBSCANCluster:\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "    def mean_dimensions(self):\n",
    "        self.dimensions = np.zeros(len(self.points[0].dimensions))\n",
    "        for point in self.points:\n",
    "            self.dimensions = np.add(self.dimensions,point.dimensions)\n",
    "        self.dimensions = self.dimensions/len(self.points)\n",
    "\n",
    "def dbscan(data,epsilon,M):\n",
    "    #data is a np.array\n",
    "    all_points,core_points,outliers = define_type(data,epsilon,M)\n",
    "    clusters = []\n",
    "    #form clusters\n",
    "    for core_point in core_points:\n",
    "        #Connect all core_points\n",
    "        for neighbour in core_point.neighbours:\n",
    "            if neighbour.category == 'CORE':\n",
    "                if neighbour.cluster == None:\n",
    "                    #neighbour core point still dont have a cluster_index\n",
    "                    if (core_point.cluster == None):\n",
    "                        #both of them dont have a cluster index, so assign them a new one\n",
    "                        core_point.cluster = DBSCANCluster()\n",
    "                        neighbour.cluster = core_point.cluster\n",
    "                        clusters.append(core_point.cluster)\n",
    "                        core_point.cluster.points.append(core_point)\n",
    "                        core_point.cluster.points.append(neighbour)\n",
    "                    else:\n",
    "                        #core_point already has a cluster    \n",
    "                        neighbour.cluster = core_point.cluster\n",
    "                        core_point.cluster.points.append(neighbour)\n",
    "                else:\n",
    "                    #neighbour already has a cluster\n",
    "                    if (core_point.cluster == None):\n",
    "                        #neighbour has a cluster but the core_point doesn't\n",
    "                        core_point.cluster = neighbour.cluster\n",
    "                        neighbour.cluster.points.append(core_point)\n",
    "                    else:\n",
    "                        #both of them have a cluster\n",
    "                        if (neighbour.cluster != core_point.cluster):\n",
    "                            merge_clusters(clusters,core_point.cluster,neighbour.cluster)\n",
    "                            \n",
    "\n",
    "    for core_point in core_points:\n",
    "        #Assign clusters to the border points\n",
    "        for neighbour in core_point.neighbours:\n",
    "            if neighbour.category != 'CORE':\n",
    "                neighbour.cluster = core_point.cluster\n",
    "                core_point.cluster.points.append(neighbour)\n",
    "            \n",
    "    for cluster in clusters:\n",
    "        cluster.mean_dimensions()\n",
    "    return clusters,core_points\n",
    "\n",
    "def define_type(data,epsilon,M):\n",
    "    points = []\n",
    "    core_points = []\n",
    "    outliers = []\n",
    "    for i in range(0,len(data)):\n",
    "        points.append(DBSCANpoint(data[i],i))\n",
    "    \n",
    "    for point in points:\n",
    "        search_neighbours(point,points,epsilon)\n",
    "        if point.n_neighbours >= M:\n",
    "            point.category = 'CORE'\n",
    "            core_points.append(point)\n",
    "        elif point.there_is_core_in_neighbourhood:\n",
    "            point.category = 'BORDER'\n",
    "        else:\n",
    "            point.category = 'OUTLIER'\n",
    "            outliers.append(point)\n",
    "    \n",
    "    return points,core_points,outliers\n",
    "\n",
    "def search_neighbours(DBSCANpoint, all_points,epsilon):\n",
    "    for point in all_points:\n",
    "        if (distance.euclidean(DBSCANpoint.dimensions,point.dimensions) <= epsilon):\n",
    "            DBSCANpoint.add_neighbour(point)\n",
    "            if point.category == 'CORE':\n",
    "                DBSCANpoint.there_is_core_in_neighbourhood = True\n",
    "\n",
    "def merge_clusters(clusters_list,cluster_1,cluster_2):\n",
    "    #all points of cluster 2 are gonna become part of cluster 1\n",
    "    for point in cluster_2.points:\n",
    "        point.cluster = cluster_1\n",
    "        cluster_1.points.append(point)\n",
    "\n",
    "    clusters_list.remove(cluster_2)\n",
    "\n",
    "def assign_new_point(DBSCANpoint,core_points, epsilon):\n",
    "    shortest_dist = 9999\n",
    "    for core_point in core_points:\n",
    "        tmp_dist = distance.euclidean(DBSCANpoint.dimensions,core_point.dimensions)\n",
    "        if (tmp_dist <= epsilon):\n",
    "            if (tmp_dist < shortest_dist):\n",
    "                closest_core = core_point\n",
    "    \n",
    "    core_point.cluster.append(DBSCANpoint)\n",
    "    DBSCANpoint.cluster = core_point.cluster\n",
    "\n",
    "def test_new_data(new_data,core_points,epsilon):\n",
    "    for point in new_data:\n",
    "        assign_new_point(point,core_points,epsilon)\n",
    "\n",
    "   \n",
    "\n",
    "def dbscan_plot_2d_data(clusters):\n",
    "    colors = [\"green\",\"blue\",\"yellow\",\"pink\",\"orange\",\"purple\",\"beige\",\"brown\",\"gray\",\"cyan\",\"magenta\", \"black\"]\n",
    "    for cluster in clusters:\n",
    "        plt.plot(cluster.dimensions[0], cluster.dimensions[1], color = 'red', marker = 'o')\n",
    "        for point in cluster.points:\n",
    "            plt.scatter(point.dimensions[0], point.dimensions[1], color = colors[clusters.index(point.cluster)], alpha = 0.5)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "def plot_different_epsilons(min_epsilon, max_epsilon, step, data, M):\n",
    "    silhouette_list = []\n",
    "    epsilon_list = []\n",
    "    epsilon_tmp = min_epsilon\n",
    "    while epsilon_tmp <= max_epsilon:\n",
    "        print('-----------------------------------------------------------------')\n",
    "        print('For epsilon = ',epsilon_tmp)\n",
    "        tic = perf_counter()\n",
    "        clusters_tmp,_ = dbscan(data, epsilon_tmp, M)\n",
    "        toc = perf_counter()\n",
    "        total_time = (toc-tic)*1000\n",
    "        print (f\"It took {total_time:.2f}ms to run the algorithm.\")\n",
    "        print('Total: ',len(clusters_tmp),' clusters')\n",
    "        for i,cluster in enumerate(clusters_tmp):\n",
    "            print('Tamanho do cluster ', i+1, 'é: ',len(cluster.points))\n",
    "        clusters_tmp = silhouette_analysis(clusters_tmp)\n",
    "        silhouette_list.append(plot_silhouette_analysis(clusters_tmp))\n",
    "        epsilon_list.append(epsilon_tmp)\n",
    "        epsilon_tmp += step\n",
    "    plt.plot(epsilon_list, silhouette_list, color = 'blue', marker = 'o', alpha = 0.5)\n",
    "    plt.xlabel('Epsilon Value')\n",
    "    plt.ylabel('Average Silhouette value')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "source": [
    "Por conta do DBSCAN depende de dois hiperparâmetros, não é possível aplicar o método elbow como no k-means para encontrar bons valores de M e epsilon. Todavia, uma boa estimativa para o valor de M é a quantidade de dimensoes dos dados, porém foram feitas várias tentativas com o valor de M calculado dessa maneira, mas sem sucesso pois os dados acabavam todos dentro do mesmo cluster, sendo assim M = 2.(dimensão dos dados) foi escolhido. Sendo assim, é possível plotar gráficos para diferentes valores de epsilon e ver a qualidade dos clusters resultantes, a métrica utilizada para medir essa qualidade foi o Coeficiente de Silhouette, já que esse coeficiente engloba coesão e separação.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "M = 2*train_set_normalized.shape[1]\n",
    "print(\"M = \",M)\n",
    "epsilon = 0.005\n",
    "clusters_dbscan_2d,_ = dbscan(train_set_normalized, epsilon, M)\n",
    "toc = perf_counter()\n",
    "time_dbscan_2d = (toc-tic)*1000\n",
    "print (f\"It took {time_dbscan_2d:.2f}ms to run the algorithm.\")\n",
    "for i,cluster in enumerate(clusters_dbscan_2d):\n",
    "    print('Tamanho do cluster ', i, 'é: ',len(cluster.points))\n",
    "dbscan_plot_2d_data(clusters_dbscan_2d)\n",
    "clusters_dbscan_2d = silhouette_analysis(clusters_dbscan_2d)\n",
    "plot_different_epsilons(0.002,0.01,0.001,train_set_normalized,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 2*document_model_train_data_normalized.shape[1]\n",
    "print(\"M = \",M)\n",
    "plot_different_epsilons(0.001,0.001,0.01,document_model_train_data_normalized,M)"
   ]
  },
  {
   "source": [
    "Percebe-se então que o algoritmo não conseguiu encontrar clusters para nenhum dos valores de epsilon tentados. Isso pode ter acontecido por conta dos dados dos tweets estarem todos aglomerados próximos uns dos outros, fazendo com que algoritmos como o k-means e o DBSCAN não consiga separá-los em clusters menores, nesse caso, técnicas diferentes podem ter um melhor resultado, como as baseadas em probabilidade ou hierarquia. Isso mostra como uma heurpistica não é garantia de nada, e sim somente um guia.\n",
    "\n",
    "Além disso, o tempo de execução do DBSCAN foi extremamente alto para os tweets, especialmente por conta do alto número de dimensões dos dados, fazendo com que não fosse possível realizar testes para um grande número de combinações de M e epsilon, os testes realizados já exigiram mais de 15 horas de processamento. Sendo assim, uma melhora neste quesito é esperada ao aplicar uma redução de dimensionalidade nos dados.\n",
    "\n",
    "Também, essa \"aglomeração\" dos tweets pode ter sido causada por uma má representação vetorial dos mesmos. Apesar de terem sido experimentados diferentes modelos de transformação, o ideal seria treinar um modelo próprio em um corpus maior, 10000 tweets não é um número muito grande de dados, principalmente por serem textos curtos, onde muitos caracteres ainda são removidos na etapa de pré processamento. Um exemplo de experimento que poderia ser feito e que provavelmente teria melhores resultados é treinar um modelo de representação baseado em atenção, como BERT, onde algumas etapas do pré processamento poderiam até ser desconsideradas, já que esse tipo de modelo pode captar o significado de termos antes considerados como irrelevantes, como algumas stopwords."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Parte 2 - Redução de dimensionalidade"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A biblioteca scikit lean foi utilizada para aplicar PCA aos dados. Vamos primeiro observar os dados graficamente ao reduzi-los para 2 e 3 dimensões."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def red_dim(data,n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(data)\n",
    "    new_data = pca.transform(data)\n",
    "    print(\"Dimensionalidade reduzida, proporção de variância em cada componente é:\")\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    return new_data\n",
    "\n",
    "#\n",
    "tweets_new_2D = red_dim(train_set_normalized,2)\n",
    "scatter_data_2D(tweets_new_2D)\n",
    "tweets_new_3D = red_dim(train_set_normalized,3)\n",
    "scatter_data_3D(tweets_new_3D)"
   ]
  },
  {
   "source": [
    "Percebe-se, então, que por volta de 58% da informação dos dados pode ser representada utilizando somente 1 feature, sendo assim, o resultado das técnicas de clusterização não devem mudar muito entre dados após redução para 1 dimensão, 2 dimensões e 3 dimensões. Portanto, será aplicado o k-means nos dados reduzidos para 1,2 e 3 dimensões, onde primeiramente será aplicado o método do elbow para encontrar um bom valor para k.\n",
    "\n",
    "Ao observar a representação gráficas destes dados após redução para 2D ou 3D, é possível perceber a presença de um cluster pequeno, que não foi identificado tanto pelo k-means quanto pelo dbscan"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_new_1D = red_dim(document_model_data_normalized,1)\n",
    "tweets_new_1D_train,tweets_new_1D_test = split_dataset(tweets_new_1D,0.9,0.1)\n",
    "tweets_new_2D = red_dim(document_model_data_normalized,2)\n",
    "tweets_new_2D_train,tweets_new_2D_test = split_dataset(tweets_new_2D,0.9,0.1)\n",
    "tweets_new_3D = red_dim(document_model_data_normalized,3)\n",
    "tweets_new_3D_train,tweets_new_3D_test = split_dataset(tweets_new_3D,0.9,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(tweets_new_1D_train)):\n",
    "    points.append(Point(tweets_new_1D_train[i],i, -1))\n",
    "\n",
    "elbow_method(1,6,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(tweets_new_1D_train)):\n",
    "    points.append(Point(tweets_new_1D_train[i],i, -1))\n",
    "numero_de_clusters = 4\n",
    "tic = perf_counter()\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_1d_kmeans = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_1d_kmeans:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "print_word_cloud(clusters,all_tweets)\n",
    "clusters = silhouette_analysis(clusters)\n",
    "avg_sa_tweets_1d = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados normalizador com normalize\n",
    "points = []\n",
    "for i in range(len(tweets_new_2D_train)):\n",
    "    points.append(Point(tweets_new_2D_train[i],i, -1))\n",
    "\n",
    "elbow_method(1,6,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(tweets_new_2D_train)):\n",
    "    points.append(Point(tweets_new_2D_train[i],i, -1))\n",
    "numero_de_clusters = 3\n",
    "tic = perf_counter()\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_2d_kmeans = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_2d_kmeans:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "clusters = silhouette_analysis(clusters)\n",
    "avg_sa_tweets_2d = plot_silhouette_analysis(clusters)\n",
    "plot_2d_data(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(tweets_new_3D_train)):\n",
    "    points.append(Point(tweets_new_3D_train[i],i, -1))\n",
    "\n",
    "elbow_method(1,6,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(len(tweets_new_3D_train)):\n",
    "    points.append(Point(tweets_new_3D_train[i],i, -1))\n",
    "numero_de_clusters = 2\n",
    "tic = perf_counter()\n",
    "clusters = k_means(points, numero_de_clusters)\n",
    "toc = perf_counter()\n",
    "time_tweets_3d_kmeans = (toc-tic)*1000\n",
    "print (f\"It took {time_tweets_3d_kmeans:.2f}ms to run the algorithm.\")\n",
    "cost = cost_evaluation(clusters)\n",
    "print(cost)\n",
    "clusters = silhouette_analysis(clusters)\n",
    "avg_sa_tweets_3d = plot_silhouette_analysis(clusters)"
   ]
  },
  {
   "source": [
    "Testando agora o dbscan, como o número de dimensões neste caso é bem pequeno, então M será estimado como um valor maior para evitar a inclusão de outliers nos clusters.\n",
    "\n",
    "Dados em 1D"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "M = 50\n",
    "print(\"M = \",M)\n",
    "epsilon = 0.005\n",
    "clusters_dbscan_tweets_1D,_ = dbscan(tweets_new_1D_train, epsilon, M)\n",
    "toc = perf_counter()\n",
    "time_dbscan_tweets_1d = (toc-tic)*1000\n",
    "print (f\"It took {time_dbscan_tweets_1d:.2f}ms to run the algorithm.\")\n",
    "print_word_cloud(clusters_dbscan_tweets_1D,all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_word_cloud(clusters_dbscan_tweets_1D,all_tweets)"
   ]
  },
  {
   "source": [
    "Dados em 2D"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "M = 2*tweets_new_2D_train.shape[1]\n",
    "print(\"M = \",M)\n",
    "epsilon = 0.005\n",
    "clusters_dbscan_tweets_2D,_ = dbscan(tweets_new_2D_train, epsilon, M)\n",
    "toc = perf_counter()\n",
    "time_dbscan_tweets_2d = (toc-tic)*1000\n",
    "print (f\"It took {time_dbscan_tweets_2d:.2f}ms to run the algorithm.\")\n",
    "print_word_cloud(clusters_dbscan_tweets_2D,all_tweets)"
   ]
  },
  {
   "source": [
    "Dados em 3D"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "M = 2*tweets_new_3D_train.shape[1]\n",
    "print(\"M = \",M)\n",
    "epsilon = 0.005\n",
    "clusters_dbscan_tweets_3D,_ = dbscan(tweets_new_3D_train, epsilon, M)\n",
    "toc = perf_counter()\n",
    "time_dbscan_tweets_3d = (toc-tic)*1000\n",
    "print (f\"It took {time_dbscan_tweets_3d:.2f}ms to run the algorithm.\")\n",
    "print_word_cloud(clusters_dbscan_tweets_3D,all_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd030e434770c8343143e074606db59223f9c01b362ce1f6346061549c91b66ccf6",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}